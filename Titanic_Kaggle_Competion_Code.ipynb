{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dca89bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2ba93163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick way to fill in some missing values\n",
    "train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna('S')\n",
    "test_df = test_df.fillna(test_df['Fare'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a49cfd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data parameterization\n",
    "def parameterizeData(data):\n",
    "    data[\"Sex\"] = [1 if x==\"male\" else 0 for x in data[\"Sex\"]]\n",
    "    data[\"EmbarkS\"] = [1 if x==\"S\" else 0 for x in data[\"Embarked\"]]\n",
    "    data[\"EmbarkC\"] = [1 if x==\"C\" else 0 for x in data[\"Embarked\"]]\n",
    "    data[\"EmbarkQ\"] = [1 if x==\"Q\" else 0 for x in data[\"Embarked\"]]\n",
    "    data[\"Master\"] = [1 if \"Master.\" in x else 0 for x in data[\"Name\"]]\n",
    "    data[\"Mrs\"] = [1 if \"Mrs.\" in x else 0 for x in data[\"Name\"]]\n",
    "    data[\"Miss\"] = [1 if \"Miss.\" in x else 0 for x in data[\"Name\"]]\n",
    "    data[\"Mr\"] = [1 if \"Mr.\" in x else 0 for x in data[\"Name\"]]\n",
    "    data = data.drop(columns = ['Embarked','Name','Cabin','Ticket'])\n",
    "    return data\n",
    "\n",
    "train_df_par = parameterizeData(train_df)\n",
    "test_df_par = parameterizeData(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "54134fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict missing age values\n",
    "\n",
    "#Get training, validation, and test values\n",
    "total_df = [train_df_par, test_df_par]\n",
    "age_df = pd.concat(total_df)\n",
    "\n",
    "age_df = age_df[['PassengerId','Pclass','SibSp','Parch','Master','Mrs','Miss', 'Mr','Age']]\n",
    "age_df = age_df.sample(frac = 1)\n",
    "\n",
    "age_X_test = age_df[age_df[\"Age\"].isnull()]\n",
    "age_X = age_df[age_df[\"Age\"].isnull() == False]\n",
    "\n",
    "age_X_val = age_X[math.ceil(len(age_X)*.8):]\n",
    "age_X = age_X[:math.ceil(len(age_X)*.8)]\n",
    "\n",
    "age_y = age_X.pop(\"Age\")\n",
    "age_y_val = age_X_val.pop(\"Age\")\n",
    "\n",
    "age_X = age_X.drop(columns = 'PassengerId')\n",
    "age_X_val = age_X_val.drop(columns = 'PassengerId')\n",
    "age_X_test = age_X_test.drop(columns = \"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8544d95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "29/29 [==============================] - 1s 6ms/step - loss: 1137.2893 - val_loss: 964.4719\n",
      "Epoch 2/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 1111.8801 - val_loss: 937.8444\n",
      "Epoch 3/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 1080.6044 - val_loss: 903.7625\n",
      "Epoch 4/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 1037.2638 - val_loss: 856.2123\n",
      "Epoch 5/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 976.1766 - val_loss: 790.0475\n",
      "Epoch 6/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 894.8761 - val_loss: 704.4019\n",
      "Epoch 7/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 791.6276 - val_loss: 598.8971\n",
      "Epoch 8/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 667.3423 - val_loss: 480.4234\n",
      "Epoch 9/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 533.2778 - val_loss: 361.8852\n",
      "Epoch 10/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 403.1772 - val_loss: 262.3137\n",
      "Epoch 11/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 298.8168 - val_loss: 190.7512\n",
      "Epoch 12/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 226.7052 - val_loss: 150.6848\n",
      "Epoch 13/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 184.7681 - val_loss: 132.7059\n",
      "Epoch 14/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 163.3410 - val_loss: 125.8298\n",
      "Epoch 15/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 152.2227 - val_loss: 122.3161\n",
      "Epoch 16/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 145.6794 - val_loss: 120.2540\n",
      "Epoch 17/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 141.5372 - val_loss: 119.5580\n",
      "Epoch 18/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 138.2445 - val_loss: 118.3132\n",
      "Epoch 19/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 136.0951 - val_loss: 116.8844\n",
      "Epoch 20/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 134.1710 - val_loss: 117.0691\n",
      "Epoch 21/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 132.6612 - val_loss: 116.4427\n",
      "Epoch 22/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 131.5205 - val_loss: 115.8505\n",
      "Epoch 23/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 130.5960 - val_loss: 115.1479\n",
      "Epoch 24/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 129.5975 - val_loss: 114.9540\n",
      "Epoch 25/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 129.0934 - val_loss: 113.8298\n",
      "Epoch 26/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 128.1541 - val_loss: 114.5367\n",
      "Epoch 27/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 127.3990 - val_loss: 114.1653\n",
      "Epoch 28/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 126.8293 - val_loss: 113.3083\n",
      "Epoch 29/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 126.4101 - val_loss: 112.8278\n",
      "Epoch 30/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 125.9534 - val_loss: 112.2398\n",
      "Epoch 31/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 125.3768 - val_loss: 112.0657\n",
      "Epoch 32/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 124.9303 - val_loss: 112.2066\n",
      "Epoch 33/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 124.8843 - val_loss: 111.8695\n",
      "Epoch 34/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 124.2537 - val_loss: 110.4645\n",
      "Epoch 35/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 123.8920 - val_loss: 110.8464\n",
      "Epoch 36/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 123.5856 - val_loss: 110.6134\n",
      "Epoch 37/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 123.1047 - val_loss: 109.1842\n",
      "Epoch 38/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 122.8328 - val_loss: 109.1044\n",
      "Epoch 39/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 122.3989 - val_loss: 109.4983\n",
      "Epoch 40/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 122.1479 - val_loss: 109.4024\n",
      "Epoch 41/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 121.9737 - val_loss: 108.5827\n",
      "Epoch 42/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 121.6821 - val_loss: 109.1319\n",
      "Epoch 43/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 121.5408 - val_loss: 107.8287\n",
      "Epoch 44/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 121.0162 - val_loss: 108.4009\n",
      "Epoch 45/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 120.7423 - val_loss: 107.4839\n",
      "Epoch 46/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 120.4432 - val_loss: 107.0126\n",
      "Epoch 47/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 120.3739 - val_loss: 107.0321\n",
      "Epoch 48/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 120.0549 - val_loss: 106.5720\n",
      "Epoch 49/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 119.8329 - val_loss: 107.2477\n",
      "Epoch 50/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 119.8886 - val_loss: 106.3991\n",
      "Epoch 51/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 119.3251 - val_loss: 106.2970\n",
      "Epoch 52/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 119.1050 - val_loss: 105.8381\n",
      "Epoch 53/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 118.9419 - val_loss: 105.9376\n",
      "Epoch 54/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 118.7108 - val_loss: 104.6664\n",
      "Epoch 55/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 118.5393 - val_loss: 105.0264\n",
      "Epoch 56/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 118.2920 - val_loss: 105.1521\n",
      "Epoch 57/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 118.2703 - val_loss: 105.0631\n",
      "Epoch 58/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 118.0034 - val_loss: 104.5051\n",
      "Epoch 59/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 117.7096 - val_loss: 104.2925\n",
      "Epoch 60/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 117.5899 - val_loss: 104.0746\n",
      "Epoch 61/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 117.4055 - val_loss: 104.1519\n",
      "Epoch 62/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 117.2227 - val_loss: 103.6403\n",
      "Epoch 63/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 117.1806 - val_loss: 104.0934\n",
      "Epoch 64/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 116.9255 - val_loss: 103.6224\n",
      "Epoch 65/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 116.8876 - val_loss: 103.2571\n",
      "Epoch 66/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 116.6392 - val_loss: 103.7881\n",
      "Epoch 67/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 116.4276 - val_loss: 103.2174\n",
      "Epoch 68/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 116.6587 - val_loss: 102.9982\n",
      "Epoch 69/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 116.1703 - val_loss: 103.3130\n",
      "Epoch 70/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 116.0494 - val_loss: 102.7375\n",
      "Epoch 71/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 116.0432 - val_loss: 102.4898\n",
      "Epoch 72/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 115.9899 - val_loss: 103.2200\n",
      "Epoch 73/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 115.7126 - val_loss: 102.4857\n",
      "Epoch 74/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 115.6994 - val_loss: 102.6816\n",
      "Epoch 75/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 115.4034 - val_loss: 102.1597\n",
      "Epoch 76/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 115.4726 - val_loss: 101.9893\n",
      "Epoch 77/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 115.3196 - val_loss: 102.5966\n",
      "Epoch 78/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 115.2258 - val_loss: 101.6240\n",
      "Epoch 79/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 2ms/step - loss: 115.0696 - val_loss: 101.8278\n",
      "Epoch 80/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 115.0915 - val_loss: 101.9463\n",
      "Epoch 81/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 114.9681 - val_loss: 101.2811\n",
      "Epoch 82/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 114.7823 - val_loss: 101.6898\n",
      "Epoch 83/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 114.8525 - val_loss: 101.4323\n",
      "Epoch 84/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 114.5418 - val_loss: 101.0862\n",
      "Epoch 85/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 114.5548 - val_loss: 100.8065\n",
      "Epoch 86/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 114.2965 - val_loss: 101.2530\n",
      "Epoch 87/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 114.1894 - val_loss: 101.1748\n",
      "Epoch 88/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 114.0605 - val_loss: 100.6378\n",
      "Epoch 89/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 114.1319 - val_loss: 100.3125\n",
      "Epoch 90/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 113.9566 - val_loss: 100.8389\n",
      "Epoch 91/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 113.9929 - val_loss: 101.0665\n",
      "Epoch 92/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 113.7981 - val_loss: 100.2760\n",
      "Epoch 93/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 113.8396 - val_loss: 100.8149\n",
      "Epoch 94/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 114.0444 - val_loss: 99.9347\n",
      "Epoch 95/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 113.6941 - val_loss: 100.8372\n",
      "Epoch 96/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 113.8612 - val_loss: 100.0877\n",
      "Epoch 97/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 113.5879 - val_loss: 100.4382\n",
      "Epoch 98/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 113.7909 - val_loss: 100.5619\n",
      "Epoch 99/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 113.1807 - val_loss: 99.9129\n",
      "Epoch 100/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 113.6928 - val_loss: 100.2731\n",
      "Epoch 101/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 113.4313 - val_loss: 100.4481\n",
      "Epoch 102/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 113.0954 - val_loss: 100.1447\n",
      "Epoch 103/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 113.2809 - val_loss: 100.1323\n",
      "Epoch 104/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 113.2708 - val_loss: 100.1272\n",
      "Epoch 105/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 112.8635 - val_loss: 99.5966\n",
      "Epoch 106/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.7961 - val_loss: 99.7498\n",
      "Epoch 107/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.7471 - val_loss: 99.7825\n",
      "Epoch 108/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.7149 - val_loss: 99.9120\n",
      "Epoch 109/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.6555 - val_loss: 100.0367\n",
      "Epoch 110/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.6183 - val_loss: 100.0651\n",
      "Epoch 111/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 112.5247 - val_loss: 99.3612\n",
      "Epoch 112/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.5706 - val_loss: 99.2588\n",
      "Epoch 113/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.4186 - val_loss: 99.9386\n",
      "Epoch 114/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.4407 - val_loss: 99.3986\n",
      "Epoch 115/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.3448 - val_loss: 99.2619\n",
      "Epoch 116/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.2914 - val_loss: 99.4631\n",
      "Epoch 117/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 112.2260 - val_loss: 98.3755\n",
      "Epoch 118/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.3242 - val_loss: 99.3735\n",
      "Epoch 119/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.3435 - val_loss: 98.4709\n",
      "Epoch 120/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.0642 - val_loss: 99.0739\n",
      "Epoch 121/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.0605 - val_loss: 98.8112\n",
      "Epoch 122/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.2245 - val_loss: 99.2980\n",
      "Epoch 123/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 112.1935 - val_loss: 98.5647\n",
      "Epoch 124/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.8830 - val_loss: 98.9999\n",
      "Epoch 125/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.8307 - val_loss: 99.1639\n",
      "Epoch 126/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.8003 - val_loss: 98.5833\n",
      "Epoch 127/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.8132 - val_loss: 98.4814\n",
      "Epoch 128/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.7656 - val_loss: 98.8609\n",
      "Epoch 129/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.8068 - val_loss: 99.0854\n",
      "Epoch 130/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 111.6165 - val_loss: 98.3330\n",
      "Epoch 131/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.6222 - val_loss: 98.6695\n",
      "Epoch 132/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.9511 - val_loss: 98.8497\n",
      "Epoch 133/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.5140 - val_loss: 98.5344\n",
      "Epoch 134/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 111.4884 - val_loss: 98.1906\n",
      "Epoch 135/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.2588 - val_loss: 98.9216\n",
      "Epoch 136/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.3585 - val_loss: 98.5554\n",
      "Epoch 137/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.4641 - val_loss: 98.6600\n",
      "Epoch 138/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.4348 - val_loss: 97.4089\n",
      "Epoch 139/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.3053 - val_loss: 97.8480\n",
      "Epoch 140/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.1704 - val_loss: 98.0920\n",
      "Epoch 141/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.3102 - val_loss: 97.6536\n",
      "Epoch 142/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.1339 - val_loss: 98.1408\n",
      "Epoch 143/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.1556 - val_loss: 98.5227\n",
      "Epoch 144/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.2927 - val_loss: 97.8674\n",
      "Epoch 145/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.4603 - val_loss: 98.3849\n",
      "Epoch 146/2000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 111.1659 - val_loss: 97.3678\n",
      "Epoch 147/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.8912 - val_loss: 98.0675\n",
      "Epoch 148/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.9552 - val_loss: 97.8415\n",
      "Epoch 149/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.9823 - val_loss: 98.0833\n",
      "Epoch 150/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 111.0330 - val_loss: 97.4867\n",
      "Epoch 151/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.7779 - val_loss: 96.8107\n",
      "Epoch 152/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.7560 - val_loss: 97.4079\n",
      "Epoch 153/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.7710 - val_loss: 97.8052\n",
      "Epoch 154/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.6105 - val_loss: 97.5023\n",
      "Epoch 155/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.9025 - val_loss: 97.9561\n",
      "Epoch 156/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.6258 - val_loss: 97.1859\n",
      "Epoch 157/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 2ms/step - loss: 110.5105 - val_loss: 97.5555\n",
      "Epoch 158/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.6477 - val_loss: 97.5618\n",
      "Epoch 159/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.5648 - val_loss: 97.3218\n",
      "Epoch 160/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.4774 - val_loss: 98.0935\n",
      "Epoch 161/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.3745 - val_loss: 97.8769\n",
      "Epoch 162/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.3745 - val_loss: 98.1120\n",
      "Epoch 163/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.4262 - val_loss: 97.5603\n",
      "Epoch 164/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.4196 - val_loss: 97.2721\n",
      "Epoch 165/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.3911 - val_loss: 97.8424\n",
      "Epoch 166/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.3216 - val_loss: 97.1477\n",
      "Epoch 167/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.1299 - val_loss: 97.9868\n",
      "Epoch 168/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.5515 - val_loss: 98.3966\n",
      "Epoch 169/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.0616 - val_loss: 97.2300\n",
      "Epoch 170/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.9967 - val_loss: 98.3537\n",
      "Epoch 171/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.0395 - val_loss: 97.7308\n",
      "Epoch 172/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.0686 - val_loss: 98.1206\n",
      "Epoch 173/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.2126 - val_loss: 97.1933\n",
      "Epoch 174/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.9540 - val_loss: 97.1508\n",
      "Epoch 175/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.9192 - val_loss: 97.9208\n",
      "Epoch 176/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.8404 - val_loss: 97.2626\n",
      "Epoch 177/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.9879 - val_loss: 97.8330\n",
      "Epoch 178/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.9087 - val_loss: 96.8990\n",
      "Epoch 179/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.0230 - val_loss: 97.5325\n",
      "Epoch 180/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.9427 - val_loss: 97.5120\n",
      "Epoch 181/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 110.0045 - val_loss: 97.1463\n",
      "Epoch 182/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.9282 - val_loss: 97.7010\n",
      "Epoch 183/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.6378 - val_loss: 97.6214\n",
      "Epoch 184/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.6572 - val_loss: 96.8116\n",
      "Epoch 185/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.5941 - val_loss: 97.5426\n",
      "Epoch 186/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.6122 - val_loss: 97.4573\n",
      "Epoch 187/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.7872 - val_loss: 97.7235\n",
      "Epoch 188/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.6076 - val_loss: 97.1519\n",
      "Epoch 189/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.7776 - val_loss: 97.4974\n",
      "Epoch 190/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.3764 - val_loss: 97.0298\n",
      "Epoch 191/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.3898 - val_loss: 97.1288\n",
      "Epoch 192/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.4236 - val_loss: 97.3848\n",
      "Epoch 193/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.5818 - val_loss: 96.8746\n",
      "Epoch 194/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.2914 - val_loss: 97.5018\n",
      "Epoch 195/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.4013 - val_loss: 96.8355\n",
      "Epoch 196/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.3911 - val_loss: 97.8463\n",
      "Epoch 197/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.1447 - val_loss: 97.3294\n",
      "Epoch 198/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.2324 - val_loss: 97.1202\n",
      "Epoch 199/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.2614 - val_loss: 97.0560\n",
      "Epoch 200/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.1987 - val_loss: 97.5271\n",
      "Epoch 201/2000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 109.1873 - val_loss: 97.1949\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create and train age model\n",
    "age_normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "age_normalizer.adapt(age_X)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "model = tf.keras.Sequential([age_normalizer,\n",
    "                            tf.keras.layers.Dense(100,activation=\"relu\"),\n",
    "                            tf.keras.layers.Dense(100,activation=\"relu\"),\n",
    "                            tf.keras.layers.Dense(100,activation=\"relu\"),\n",
    "                            tf.keras.layers.Dense(1,activation=\"relu\")])\n",
    "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.0001))\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"age_checkpoint.h5\", save_best_only=True)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=50)\n",
    "\n",
    "history = model.fit(age_X, age_y, validation_data=(age_X_val, age_y_val), epochs=2000, batch_size=BATCH_SIZE,\n",
    "          callbacks=[model_checkpoint,early_stopping])\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.axis([0, 500, 0, 200])\n",
    "\n",
    "model.load_weights('age_checkpoint.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "24de3dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 998us/step\n"
     ]
    }
   ],
   "source": [
    "#Use model to fill in missing age values\n",
    "age_predictions = model.predict(age_X_test.drop(columns = 'PassengerId'))\n",
    "for i in range(len(age_predictions)):\n",
    "    PId = age_X_test['PassengerId'].values[i]\n",
    "    for j in range(len(train_df_par)):\n",
    "        if PId == train_df_par.at[j,\"PassengerId\"]:\n",
    "            train_df_par.at[j,'Age'] = age_predictions[i]\n",
    "    for j in range(len(test_df_par)):\n",
    "        if PId == test_df_par.at[j,\"PassengerId\"]:\n",
    "            test_df_par.at[j,'Age'] = age_predictions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2f504e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get training, validation, and test data\n",
    "\n",
    "X = train_df_par \n",
    "X_test = test_df_par\n",
    "\n",
    "X = X.sample(frac = 1)\n",
    "\n",
    "X_val = X[math.ceil(len(X)*.8):]\n",
    "X = X[:math.ceil(len(X)*.8)]\n",
    "\n",
    "y = X.pop('Survived')\n",
    "y_val = X_val.pop('Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0967488d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "23/23 [==============================] - 1s 9ms/step - loss: 0.7057 - accuracy: 0.3927 - val_loss: 0.7125 - val_accuracy: 0.3708\n",
      "Epoch 2/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7006 - accuracy: 0.4348 - val_loss: 0.7090 - val_accuracy: 0.3820\n",
      "Epoch 3/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6960 - accuracy: 0.4628 - val_loss: 0.7052 - val_accuracy: 0.3989\n",
      "Epoch 4/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.4923 - val_loss: 0.7018 - val_accuracy: 0.4326\n",
      "Epoch 5/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5428 - val_loss: 0.6986 - val_accuracy: 0.4607\n",
      "Epoch 6/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6826 - accuracy: 0.5919 - val_loss: 0.6953 - val_accuracy: 0.5112\n",
      "Epoch 7/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6783 - accuracy: 0.6592 - val_loss: 0.6920 - val_accuracy: 0.5730\n",
      "Epoch 8/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6741 - accuracy: 0.6844 - val_loss: 0.6888 - val_accuracy: 0.5955\n",
      "Epoch 9/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6700 - accuracy: 0.6971 - val_loss: 0.6856 - val_accuracy: 0.6067\n",
      "Epoch 10/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6658 - accuracy: 0.7041 - val_loss: 0.6823 - val_accuracy: 0.6124\n",
      "Epoch 11/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6618 - accuracy: 0.7181 - val_loss: 0.6792 - val_accuracy: 0.6348\n",
      "Epoch 12/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6578 - accuracy: 0.7251 - val_loss: 0.6761 - val_accuracy: 0.6404\n",
      "Epoch 13/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6539 - accuracy: 0.7321 - val_loss: 0.6730 - val_accuracy: 0.6517\n",
      "Epoch 14/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6499 - accuracy: 0.7335 - val_loss: 0.6702 - val_accuracy: 0.6685\n",
      "Epoch 15/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6460 - accuracy: 0.7405 - val_loss: 0.6670 - val_accuracy: 0.6685\n",
      "Epoch 16/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6422 - accuracy: 0.7433 - val_loss: 0.6641 - val_accuracy: 0.6685\n",
      "Epoch 17/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6383 - accuracy: 0.7475 - val_loss: 0.6610 - val_accuracy: 0.6685\n",
      "Epoch 18/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6345 - accuracy: 0.7504 - val_loss: 0.6581 - val_accuracy: 0.6798\n",
      "Epoch 19/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6307 - accuracy: 0.7532 - val_loss: 0.6553 - val_accuracy: 0.6854\n",
      "Epoch 20/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6270 - accuracy: 0.7588 - val_loss: 0.6525 - val_accuracy: 0.6798\n",
      "Epoch 21/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6232 - accuracy: 0.7602 - val_loss: 0.6497 - val_accuracy: 0.6910\n",
      "Epoch 22/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6195 - accuracy: 0.7644 - val_loss: 0.6467 - val_accuracy: 0.7022\n",
      "Epoch 23/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6158 - accuracy: 0.7700 - val_loss: 0.6437 - val_accuracy: 0.7022\n",
      "Epoch 24/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6120 - accuracy: 0.7700 - val_loss: 0.6409 - val_accuracy: 0.7022\n",
      "Epoch 25/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6083 - accuracy: 0.7728 - val_loss: 0.6381 - val_accuracy: 0.7022\n",
      "Epoch 26/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6046 - accuracy: 0.7728 - val_loss: 0.6352 - val_accuracy: 0.7022\n",
      "Epoch 27/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6008 - accuracy: 0.7798 - val_loss: 0.6320 - val_accuracy: 0.7022\n",
      "Epoch 28/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5971 - accuracy: 0.7798 - val_loss: 0.6291 - val_accuracy: 0.7079\n",
      "Epoch 29/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5935 - accuracy: 0.7812 - val_loss: 0.6265 - val_accuracy: 0.7079\n",
      "Epoch 30/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5899 - accuracy: 0.7826 - val_loss: 0.6235 - val_accuracy: 0.7079\n",
      "Epoch 31/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5861 - accuracy: 0.7826 - val_loss: 0.6205 - val_accuracy: 0.7135\n",
      "Epoch 32/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5825 - accuracy: 0.7826 - val_loss: 0.6175 - val_accuracy: 0.7247\n",
      "Epoch 33/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5788 - accuracy: 0.7826 - val_loss: 0.6145 - val_accuracy: 0.7303\n",
      "Epoch 34/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5752 - accuracy: 0.7882 - val_loss: 0.6118 - val_accuracy: 0.7360\n",
      "Epoch 35/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5714 - accuracy: 0.7896 - val_loss: 0.6087 - val_accuracy: 0.7416\n",
      "Epoch 36/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5678 - accuracy: 0.7924 - val_loss: 0.6057 - val_accuracy: 0.7360\n",
      "Epoch 37/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5641 - accuracy: 0.7994 - val_loss: 0.6029 - val_accuracy: 0.7416\n",
      "Epoch 38/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5606 - accuracy: 0.8008 - val_loss: 0.6000 - val_accuracy: 0.7416\n",
      "Epoch 39/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5569 - accuracy: 0.8008 - val_loss: 0.5974 - val_accuracy: 0.7472\n",
      "Epoch 40/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5534 - accuracy: 0.8022 - val_loss: 0.5945 - val_accuracy: 0.7472\n",
      "Epoch 41/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5498 - accuracy: 0.8008 - val_loss: 0.5917 - val_accuracy: 0.7528\n",
      "Epoch 42/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5462 - accuracy: 0.7994 - val_loss: 0.5889 - val_accuracy: 0.7528\n",
      "Epoch 43/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5427 - accuracy: 0.7994 - val_loss: 0.5860 - val_accuracy: 0.7472\n",
      "Epoch 44/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5392 - accuracy: 0.7980 - val_loss: 0.5835 - val_accuracy: 0.7472\n",
      "Epoch 45/2000\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.5358 - accuracy: 0.8008 - val_loss: 0.5807 - val_accuracy: 0.7472\n",
      "Epoch 46/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.8022 - val_loss: 0.5780 - val_accuracy: 0.7472\n",
      "Epoch 47/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5291 - accuracy: 0.8022 - val_loss: 0.5756 - val_accuracy: 0.7472\n",
      "Epoch 48/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5259 - accuracy: 0.8022 - val_loss: 0.5730 - val_accuracy: 0.7528\n",
      "Epoch 49/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5227 - accuracy: 0.8022 - val_loss: 0.5704 - val_accuracy: 0.7528\n",
      "Epoch 50/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5194 - accuracy: 0.8022 - val_loss: 0.5678 - val_accuracy: 0.7528\n",
      "Epoch 51/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5163 - accuracy: 0.8022 - val_loss: 0.5652 - val_accuracy: 0.7584\n",
      "Epoch 52/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5131 - accuracy: 0.8050 - val_loss: 0.5624 - val_accuracy: 0.7584\n",
      "Epoch 53/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5099 - accuracy: 0.8065 - val_loss: 0.5602 - val_accuracy: 0.7584\n",
      "Epoch 54/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5069 - accuracy: 0.8079 - val_loss: 0.5579 - val_accuracy: 0.7640\n",
      "Epoch 55/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5039 - accuracy: 0.8093 - val_loss: 0.5551 - val_accuracy: 0.7584\n",
      "Epoch 56/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.8093 - val_loss: 0.5528 - val_accuracy: 0.7584\n",
      "Epoch 57/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4980 - accuracy: 0.8135 - val_loss: 0.5506 - val_accuracy: 0.7640\n",
      "Epoch 58/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4951 - accuracy: 0.8177 - val_loss: 0.5480 - val_accuracy: 0.7640\n",
      "Epoch 59/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4922 - accuracy: 0.8205 - val_loss: 0.5460 - val_accuracy: 0.7697\n",
      "Epoch 60/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4894 - accuracy: 0.8219 - val_loss: 0.5441 - val_accuracy: 0.7640\n",
      "Epoch 61/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4868 - accuracy: 0.8247 - val_loss: 0.5418 - val_accuracy: 0.7640\n",
      "Epoch 62/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4841 - accuracy: 0.8247 - val_loss: 0.5398 - val_accuracy: 0.7697\n",
      "Epoch 63/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4814 - accuracy: 0.8261 - val_loss: 0.5378 - val_accuracy: 0.7753\n",
      "Epoch 64/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4789 - accuracy: 0.8261 - val_loss: 0.5354 - val_accuracy: 0.7753\n",
      "Epoch 65/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4764 - accuracy: 0.8261 - val_loss: 0.5339 - val_accuracy: 0.7753\n",
      "Epoch 66/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4739 - accuracy: 0.8261 - val_loss: 0.5318 - val_accuracy: 0.7753\n",
      "Epoch 67/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4714 - accuracy: 0.8247 - val_loss: 0.5303 - val_accuracy: 0.7809\n",
      "Epoch 68/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4691 - accuracy: 0.8261 - val_loss: 0.5283 - val_accuracy: 0.7809\n",
      "Epoch 69/2000\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4669 - accuracy: 0.8261 - val_loss: 0.5266 - val_accuracy: 0.7753\n",
      "Epoch 70/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4646 - accuracy: 0.8275 - val_loss: 0.5248 - val_accuracy: 0.7809\n",
      "Epoch 71/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4626 - accuracy: 0.8275 - val_loss: 0.5234 - val_accuracy: 0.7865\n",
      "Epoch 72/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4604 - accuracy: 0.8247 - val_loss: 0.5215 - val_accuracy: 0.7865\n",
      "Epoch 73/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4584 - accuracy: 0.8233 - val_loss: 0.5198 - val_accuracy: 0.7809\n",
      "Epoch 74/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4564 - accuracy: 0.8247 - val_loss: 0.5182 - val_accuracy: 0.7809\n",
      "Epoch 75/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4544 - accuracy: 0.8247 - val_loss: 0.5166 - val_accuracy: 0.7809\n",
      "Epoch 76/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4525 - accuracy: 0.8247 - val_loss: 0.5154 - val_accuracy: 0.7865\n",
      "Epoch 77/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4507 - accuracy: 0.8247 - val_loss: 0.5137 - val_accuracy: 0.7865\n",
      "Epoch 78/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4489 - accuracy: 0.8247 - val_loss: 0.5123 - val_accuracy: 0.7865\n",
      "Epoch 79/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4471 - accuracy: 0.8233 - val_loss: 0.5109 - val_accuracy: 0.7865\n",
      "Epoch 80/2000\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4455 - accuracy: 0.8247 - val_loss: 0.5101 - val_accuracy: 0.7865\n",
      "Epoch 81/2000\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4438 - accuracy: 0.8261 - val_loss: 0.5087 - val_accuracy: 0.7865\n",
      "Epoch 82/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4422 - accuracy: 0.8247 - val_loss: 0.5071 - val_accuracy: 0.7865\n",
      "Epoch 83/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4406 - accuracy: 0.8247 - val_loss: 0.5063 - val_accuracy: 0.7865\n",
      "Epoch 84/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4392 - accuracy: 0.8261 - val_loss: 0.5049 - val_accuracy: 0.7865\n",
      "Epoch 85/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4376 - accuracy: 0.8261 - val_loss: 0.5039 - val_accuracy: 0.7809\n",
      "Epoch 86/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4361 - accuracy: 0.8261 - val_loss: 0.5031 - val_accuracy: 0.7809\n",
      "Epoch 87/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4348 - accuracy: 0.8261 - val_loss: 0.5021 - val_accuracy: 0.7809\n",
      "Epoch 88/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4334 - accuracy: 0.8261 - val_loss: 0.5011 - val_accuracy: 0.7865\n",
      "Epoch 89/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4321 - accuracy: 0.8247 - val_loss: 0.5001 - val_accuracy: 0.7809\n",
      "Epoch 90/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4309 - accuracy: 0.8247 - val_loss: 0.4991 - val_accuracy: 0.7809\n",
      "Epoch 91/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4297 - accuracy: 0.8233 - val_loss: 0.4983 - val_accuracy: 0.7753\n",
      "Epoch 92/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4284 - accuracy: 0.8261 - val_loss: 0.4976 - val_accuracy: 0.7809\n",
      "Epoch 93/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4272 - accuracy: 0.8233 - val_loss: 0.4963 - val_accuracy: 0.7753\n",
      "Epoch 94/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4261 - accuracy: 0.8247 - val_loss: 0.4955 - val_accuracy: 0.7753\n",
      "Epoch 95/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4250 - accuracy: 0.8247 - val_loss: 0.4947 - val_accuracy: 0.7753\n",
      "Epoch 96/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4239 - accuracy: 0.8261 - val_loss: 0.4939 - val_accuracy: 0.7753\n",
      "Epoch 97/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4228 - accuracy: 0.8275 - val_loss: 0.4933 - val_accuracy: 0.7753\n",
      "Epoch 98/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4218 - accuracy: 0.8261 - val_loss: 0.4925 - val_accuracy: 0.7809\n",
      "Epoch 99/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4207 - accuracy: 0.8275 - val_loss: 0.4922 - val_accuracy: 0.7809\n",
      "Epoch 100/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4197 - accuracy: 0.8261 - val_loss: 0.4914 - val_accuracy: 0.7809\n",
      "Epoch 101/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4187 - accuracy: 0.8261 - val_loss: 0.4909 - val_accuracy: 0.7809\n",
      "Epoch 102/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4178 - accuracy: 0.8261 - val_loss: 0.4899 - val_accuracy: 0.7809\n",
      "Epoch 103/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4169 - accuracy: 0.8261 - val_loss: 0.4893 - val_accuracy: 0.7809\n",
      "Epoch 104/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4160 - accuracy: 0.8275 - val_loss: 0.4891 - val_accuracy: 0.7809\n",
      "Epoch 105/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4151 - accuracy: 0.8261 - val_loss: 0.4883 - val_accuracy: 0.7809\n",
      "Epoch 106/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4143 - accuracy: 0.8261 - val_loss: 0.4876 - val_accuracy: 0.7809\n",
      "Epoch 107/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4134 - accuracy: 0.8261 - val_loss: 0.4871 - val_accuracy: 0.7809\n",
      "Epoch 108/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4126 - accuracy: 0.8261 - val_loss: 0.4869 - val_accuracy: 0.7809\n",
      "Epoch 109/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4118 - accuracy: 0.8261 - val_loss: 0.4864 - val_accuracy: 0.7809\n",
      "Epoch 110/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4111 - accuracy: 0.8261 - val_loss: 0.4858 - val_accuracy: 0.7809\n",
      "Epoch 111/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4104 - accuracy: 0.8261 - val_loss: 0.4856 - val_accuracy: 0.7809\n",
      "Epoch 112/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4096 - accuracy: 0.8275 - val_loss: 0.4855 - val_accuracy: 0.7809\n",
      "Epoch 113/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4089 - accuracy: 0.8275 - val_loss: 0.4848 - val_accuracy: 0.7865\n",
      "Epoch 114/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8275 - val_loss: 0.4849 - val_accuracy: 0.7865\n",
      "Epoch 115/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4075 - accuracy: 0.8275 - val_loss: 0.4844 - val_accuracy: 0.7865\n",
      "Epoch 116/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4068 - accuracy: 0.8275 - val_loss: 0.4840 - val_accuracy: 0.7865\n",
      "Epoch 117/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4061 - accuracy: 0.8275 - val_loss: 0.4835 - val_accuracy: 0.7865\n",
      "Epoch 118/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4054 - accuracy: 0.8303 - val_loss: 0.4832 - val_accuracy: 0.7921\n",
      "Epoch 119/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4047 - accuracy: 0.8303 - val_loss: 0.4829 - val_accuracy: 0.7978\n",
      "Epoch 120/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4041 - accuracy: 0.8303 - val_loss: 0.4827 - val_accuracy: 0.7978\n",
      "Epoch 121/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4034 - accuracy: 0.8303 - val_loss: 0.4823 - val_accuracy: 0.7978\n",
      "Epoch 122/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4028 - accuracy: 0.8317 - val_loss: 0.4821 - val_accuracy: 0.7978\n",
      "Epoch 123/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4022 - accuracy: 0.8331 - val_loss: 0.4818 - val_accuracy: 0.7978\n",
      "Epoch 124/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4016 - accuracy: 0.8317 - val_loss: 0.4818 - val_accuracy: 0.7978\n",
      "Epoch 125/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4010 - accuracy: 0.8331 - val_loss: 0.4816 - val_accuracy: 0.7978\n",
      "Epoch 126/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4004 - accuracy: 0.8331 - val_loss: 0.4814 - val_accuracy: 0.7978\n",
      "Epoch 127/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3998 - accuracy: 0.8359 - val_loss: 0.4811 - val_accuracy: 0.7978\n",
      "Epoch 128/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3993 - accuracy: 0.8373 - val_loss: 0.4808 - val_accuracy: 0.7978\n",
      "Epoch 129/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3987 - accuracy: 0.8387 - val_loss: 0.4805 - val_accuracy: 0.7978\n",
      "Epoch 130/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3981 - accuracy: 0.8401 - val_loss: 0.4805 - val_accuracy: 0.7978\n",
      "Epoch 131/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3976 - accuracy: 0.8415 - val_loss: 0.4805 - val_accuracy: 0.7978\n",
      "Epoch 132/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3971 - accuracy: 0.8415 - val_loss: 0.4805 - val_accuracy: 0.7978\n",
      "Epoch 133/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3966 - accuracy: 0.8429 - val_loss: 0.4799 - val_accuracy: 0.8034\n",
      "Epoch 134/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.8429 - val_loss: 0.4800 - val_accuracy: 0.8034\n",
      "Epoch 135/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3955 - accuracy: 0.8429 - val_loss: 0.4798 - val_accuracy: 0.8034\n",
      "Epoch 136/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3950 - accuracy: 0.8429 - val_loss: 0.4794 - val_accuracy: 0.8034\n",
      "Epoch 137/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3945 - accuracy: 0.8415 - val_loss: 0.4794 - val_accuracy: 0.8034\n",
      "Epoch 138/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3940 - accuracy: 0.8443 - val_loss: 0.4791 - val_accuracy: 0.8034\n",
      "Epoch 139/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3935 - accuracy: 0.8443 - val_loss: 0.4791 - val_accuracy: 0.8034\n",
      "Epoch 140/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8443 - val_loss: 0.4787 - val_accuracy: 0.8034\n",
      "Epoch 141/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3925 - accuracy: 0.8443 - val_loss: 0.4783 - val_accuracy: 0.8034\n",
      "Epoch 142/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3920 - accuracy: 0.8429 - val_loss: 0.4775 - val_accuracy: 0.8034\n",
      "Epoch 143/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3915 - accuracy: 0.8443 - val_loss: 0.4776 - val_accuracy: 0.8034\n",
      "Epoch 144/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3911 - accuracy: 0.8443 - val_loss: 0.4772 - val_accuracy: 0.8034\n",
      "Epoch 145/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3906 - accuracy: 0.8443 - val_loss: 0.4773 - val_accuracy: 0.8034\n",
      "Epoch 146/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3901 - accuracy: 0.8457 - val_loss: 0.4774 - val_accuracy: 0.8034\n",
      "Epoch 147/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3897 - accuracy: 0.8443 - val_loss: 0.4775 - val_accuracy: 0.8034\n",
      "Epoch 148/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3893 - accuracy: 0.8457 - val_loss: 0.4770 - val_accuracy: 0.8090\n",
      "Epoch 149/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3889 - accuracy: 0.8457 - val_loss: 0.4771 - val_accuracy: 0.8090\n",
      "Epoch 150/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.8471 - val_loss: 0.4771 - val_accuracy: 0.8090\n",
      "Epoch 151/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8485 - val_loss: 0.4770 - val_accuracy: 0.8090\n",
      "Epoch 152/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3876 - accuracy: 0.8485 - val_loss: 0.4769 - val_accuracy: 0.8090\n",
      "Epoch 153/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3871 - accuracy: 0.8485 - val_loss: 0.4768 - val_accuracy: 0.8090\n",
      "Epoch 154/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3867 - accuracy: 0.8485 - val_loss: 0.4769 - val_accuracy: 0.8090\n",
      "Epoch 155/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3863 - accuracy: 0.8485 - val_loss: 0.4766 - val_accuracy: 0.8090\n",
      "Epoch 156/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3859 - accuracy: 0.8471 - val_loss: 0.4766 - val_accuracy: 0.8090\n",
      "Epoch 157/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3855 - accuracy: 0.8485 - val_loss: 0.4762 - val_accuracy: 0.8090\n",
      "Epoch 158/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3851 - accuracy: 0.8485 - val_loss: 0.4764 - val_accuracy: 0.8090\n",
      "Epoch 159/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3847 - accuracy: 0.8471 - val_loss: 0.4765 - val_accuracy: 0.8090\n",
      "Epoch 160/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3844 - accuracy: 0.8471 - val_loss: 0.4763 - val_accuracy: 0.8090\n",
      "Epoch 161/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3840 - accuracy: 0.8471 - val_loss: 0.4767 - val_accuracy: 0.8090\n",
      "Epoch 162/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3836 - accuracy: 0.8471 - val_loss: 0.4763 - val_accuracy: 0.8090\n",
      "Epoch 163/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8471 - val_loss: 0.4765 - val_accuracy: 0.8090\n",
      "Epoch 164/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3829 - accuracy: 0.8471 - val_loss: 0.4765 - val_accuracy: 0.8090\n",
      "Epoch 165/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3825 - accuracy: 0.8471 - val_loss: 0.4765 - val_accuracy: 0.8090\n",
      "Epoch 166/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3821 - accuracy: 0.8471 - val_loss: 0.4767 - val_accuracy: 0.8090\n",
      "Epoch 167/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3818 - accuracy: 0.8471 - val_loss: 0.4764 - val_accuracy: 0.8090\n",
      "Epoch 168/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3814 - accuracy: 0.8471 - val_loss: 0.4766 - val_accuracy: 0.8090\n",
      "Epoch 169/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3811 - accuracy: 0.8471 - val_loss: 0.4766 - val_accuracy: 0.8090\n",
      "Epoch 170/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3807 - accuracy: 0.8471 - val_loss: 0.4765 - val_accuracy: 0.8090\n",
      "Epoch 171/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3804 - accuracy: 0.8471 - val_loss: 0.4766 - val_accuracy: 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3800 - accuracy: 0.8471 - val_loss: 0.4766 - val_accuracy: 0.8090\n",
      "Epoch 173/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3797 - accuracy: 0.8471 - val_loss: 0.4766 - val_accuracy: 0.8090\n",
      "Epoch 174/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3794 - accuracy: 0.8471 - val_loss: 0.4766 - val_accuracy: 0.8090\n",
      "Epoch 175/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3790 - accuracy: 0.8471 - val_loss: 0.4765 - val_accuracy: 0.8090\n",
      "Epoch 176/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3787 - accuracy: 0.8471 - val_loss: 0.4765 - val_accuracy: 0.8146\n",
      "Epoch 177/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3784 - accuracy: 0.8471 - val_loss: 0.4766 - val_accuracy: 0.8146\n",
      "Epoch 178/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3780 - accuracy: 0.8471 - val_loss: 0.4767 - val_accuracy: 0.8146\n",
      "Epoch 179/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3776 - accuracy: 0.8471 - val_loss: 0.4766 - val_accuracy: 0.8146\n",
      "Epoch 180/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3774 - accuracy: 0.8471 - val_loss: 0.4763 - val_accuracy: 0.8146\n",
      "Epoch 181/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3770 - accuracy: 0.8485 - val_loss: 0.4763 - val_accuracy: 0.8146\n",
      "Epoch 182/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3767 - accuracy: 0.8485 - val_loss: 0.4767 - val_accuracy: 0.8202\n",
      "Epoch 183/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3764 - accuracy: 0.8485 - val_loss: 0.4763 - val_accuracy: 0.8202\n",
      "Epoch 184/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8485 - val_loss: 0.4764 - val_accuracy: 0.8202\n",
      "Epoch 185/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3758 - accuracy: 0.8485 - val_loss: 0.4767 - val_accuracy: 0.8202\n",
      "Epoch 186/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3754 - accuracy: 0.8485 - val_loss: 0.4769 - val_accuracy: 0.8202\n",
      "Epoch 187/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3752 - accuracy: 0.8485 - val_loss: 0.4767 - val_accuracy: 0.8202\n",
      "Epoch 188/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3748 - accuracy: 0.8485 - val_loss: 0.4768 - val_accuracy: 0.8202\n",
      "Epoch 189/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3746 - accuracy: 0.8499 - val_loss: 0.4774 - val_accuracy: 0.8202\n",
      "Epoch 190/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3742 - accuracy: 0.8499 - val_loss: 0.4772 - val_accuracy: 0.8202\n",
      "Epoch 191/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3740 - accuracy: 0.8499 - val_loss: 0.4770 - val_accuracy: 0.8202\n",
      "Epoch 192/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3737 - accuracy: 0.8499 - val_loss: 0.4768 - val_accuracy: 0.8202\n",
      "Epoch 193/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3734 - accuracy: 0.8499 - val_loss: 0.4765 - val_accuracy: 0.8202\n",
      "Epoch 194/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3731 - accuracy: 0.8499 - val_loss: 0.4767 - val_accuracy: 0.8202\n",
      "Epoch 195/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3728 - accuracy: 0.8499 - val_loss: 0.4772 - val_accuracy: 0.8202\n",
      "Epoch 196/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3725 - accuracy: 0.8499 - val_loss: 0.4769 - val_accuracy: 0.8258\n",
      "Epoch 197/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3723 - accuracy: 0.8499 - val_loss: 0.4771 - val_accuracy: 0.8202\n",
      "Epoch 198/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3720 - accuracy: 0.8499 - val_loss: 0.4772 - val_accuracy: 0.8258\n",
      "Epoch 199/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3717 - accuracy: 0.8499 - val_loss: 0.4771 - val_accuracy: 0.8258\n",
      "Epoch 200/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3714 - accuracy: 0.8499 - val_loss: 0.4773 - val_accuracy: 0.8258\n",
      "Epoch 201/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3711 - accuracy: 0.8499 - val_loss: 0.4776 - val_accuracy: 0.8258\n",
      "Epoch 202/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3709 - accuracy: 0.8499 - val_loss: 0.4778 - val_accuracy: 0.8202\n",
      "Epoch 203/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3707 - accuracy: 0.8499 - val_loss: 0.4773 - val_accuracy: 0.8258\n",
      "Epoch 204/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.8499 - val_loss: 0.4778 - val_accuracy: 0.8202\n",
      "Epoch 205/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8485 - val_loss: 0.4780 - val_accuracy: 0.8202\n",
      "Epoch 206/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3698 - accuracy: 0.8499 - val_loss: 0.4779 - val_accuracy: 0.8202\n",
      "Epoch 207/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3696 - accuracy: 0.8485 - val_loss: 0.4781 - val_accuracy: 0.8202\n",
      "Epoch 208/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3692 - accuracy: 0.8485 - val_loss: 0.4779 - val_accuracy: 0.8202\n",
      "Epoch 209/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3690 - accuracy: 0.8499 - val_loss: 0.4775 - val_accuracy: 0.8202\n",
      "Epoch 210/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.8499 - val_loss: 0.4775 - val_accuracy: 0.8202\n",
      "Epoch 211/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8499 - val_loss: 0.4776 - val_accuracy: 0.8202\n",
      "Epoch 212/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8485 - val_loss: 0.4781 - val_accuracy: 0.8202\n",
      "Epoch 213/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8485 - val_loss: 0.4781 - val_accuracy: 0.8202\n",
      "Epoch 214/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3677 - accuracy: 0.8485 - val_loss: 0.4777 - val_accuracy: 0.8202\n",
      "Epoch 215/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3675 - accuracy: 0.8485 - val_loss: 0.4785 - val_accuracy: 0.8202\n",
      "Epoch 216/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3672 - accuracy: 0.8485 - val_loss: 0.4784 - val_accuracy: 0.8202\n",
      "Epoch 217/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3670 - accuracy: 0.8485 - val_loss: 0.4778 - val_accuracy: 0.8202\n",
      "Epoch 218/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3667 - accuracy: 0.8485 - val_loss: 0.4780 - val_accuracy: 0.8202\n",
      "Epoch 219/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3665 - accuracy: 0.8485 - val_loss: 0.4784 - val_accuracy: 0.8202\n",
      "Epoch 220/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3662 - accuracy: 0.8485 - val_loss: 0.4785 - val_accuracy: 0.8202\n",
      "Epoch 221/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3660 - accuracy: 0.8485 - val_loss: 0.4786 - val_accuracy: 0.8202\n",
      "Epoch 222/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3658 - accuracy: 0.8485 - val_loss: 0.4790 - val_accuracy: 0.8202\n",
      "Epoch 223/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3655 - accuracy: 0.8485 - val_loss: 0.4790 - val_accuracy: 0.8202\n",
      "Epoch 224/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3653 - accuracy: 0.8485 - val_loss: 0.4793 - val_accuracy: 0.8202\n",
      "Epoch 225/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3650 - accuracy: 0.8485 - val_loss: 0.4793 - val_accuracy: 0.8202\n",
      "Epoch 226/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3648 - accuracy: 0.8485 - val_loss: 0.4789 - val_accuracy: 0.8202\n",
      "Epoch 227/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3646 - accuracy: 0.8485 - val_loss: 0.4790 - val_accuracy: 0.8202\n",
      "Epoch 228/2000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3644 - accuracy: 0.8485 - val_loss: 0.4790 - val_accuracy: 0.8202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3641 - accuracy: 0.8485 - val_loss: 0.4790 - val_accuracy: 0.8202\n",
      "Epoch 230/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3639 - accuracy: 0.8485 - val_loss: 0.4794 - val_accuracy: 0.8202\n",
      "Epoch 231/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3636 - accuracy: 0.8485 - val_loss: 0.4794 - val_accuracy: 0.8202\n",
      "Epoch 232/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3635 - accuracy: 0.8485 - val_loss: 0.4799 - val_accuracy: 0.8202\n",
      "Epoch 233/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3632 - accuracy: 0.8499 - val_loss: 0.4800 - val_accuracy: 0.8202\n",
      "Epoch 234/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3630 - accuracy: 0.8499 - val_loss: 0.4801 - val_accuracy: 0.8202\n",
      "Epoch 235/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3628 - accuracy: 0.8499 - val_loss: 0.4801 - val_accuracy: 0.8202\n",
      "Epoch 236/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3625 - accuracy: 0.8499 - val_loss: 0.4802 - val_accuracy: 0.8202\n",
      "Epoch 237/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3623 - accuracy: 0.8499 - val_loss: 0.4802 - val_accuracy: 0.8202\n",
      "Epoch 238/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3621 - accuracy: 0.8499 - val_loss: 0.4803 - val_accuracy: 0.8202\n",
      "Epoch 239/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3619 - accuracy: 0.8499 - val_loss: 0.4801 - val_accuracy: 0.8202\n",
      "Epoch 240/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3618 - accuracy: 0.8499 - val_loss: 0.4808 - val_accuracy: 0.8202\n",
      "Epoch 241/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3615 - accuracy: 0.8499 - val_loss: 0.4812 - val_accuracy: 0.8202\n",
      "Epoch 242/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3613 - accuracy: 0.8499 - val_loss: 0.4812 - val_accuracy: 0.8202\n",
      "Epoch 243/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3610 - accuracy: 0.8499 - val_loss: 0.4812 - val_accuracy: 0.8202\n",
      "Epoch 244/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3608 - accuracy: 0.8513 - val_loss: 0.4814 - val_accuracy: 0.8202\n",
      "Epoch 245/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3606 - accuracy: 0.8527 - val_loss: 0.4811 - val_accuracy: 0.8202\n",
      "Epoch 246/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3604 - accuracy: 0.8527 - val_loss: 0.4811 - val_accuracy: 0.8202\n",
      "Epoch 247/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3603 - accuracy: 0.8513 - val_loss: 0.4814 - val_accuracy: 0.8202\n",
      "Epoch 248/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3600 - accuracy: 0.8513 - val_loss: 0.4816 - val_accuracy: 0.8202\n",
      "Epoch 249/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3598 - accuracy: 0.8527 - val_loss: 0.4810 - val_accuracy: 0.8202\n",
      "Epoch 250/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3596 - accuracy: 0.8527 - val_loss: 0.4814 - val_accuracy: 0.8202\n",
      "Epoch 251/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3593 - accuracy: 0.8527 - val_loss: 0.4815 - val_accuracy: 0.8202\n",
      "Epoch 252/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3591 - accuracy: 0.8541 - val_loss: 0.4816 - val_accuracy: 0.8202\n",
      "Epoch 253/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3590 - accuracy: 0.8527 - val_loss: 0.4813 - val_accuracy: 0.8202\n",
      "Epoch 254/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3588 - accuracy: 0.8527 - val_loss: 0.4816 - val_accuracy: 0.8202\n",
      "Epoch 255/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3585 - accuracy: 0.8541 - val_loss: 0.4821 - val_accuracy: 0.8202\n",
      "Epoch 256/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.8541 - val_loss: 0.4820 - val_accuracy: 0.8202\n",
      "Epoch 257/2000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3581 - accuracy: 0.8541 - val_loss: 0.4822 - val_accuracy: 0.8202\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Model declaration and training\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(X)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "model = tf.keras.Sequential([normalizer,\n",
    "                            tf.keras.layers.Dense(100, activation='relu'),\n",
    "                            tf.keras.layers.Dense(100, activation='relu'),\n",
    "                            tf.keras.layers.Dense(100, activation='relu'),\n",
    "                            tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "model.compile(tf.keras.optimizers.Adam(0.00001), loss=tf.keras.losses.BinaryCrossentropy(),metrics=['accuracy'])\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"checkpoint.h5\", save_best_only=True)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=100)\n",
    "\n",
    "history = model.fit(X, y, validation_data=(X_val, y_val), epochs=2000, batch_size=BATCH_SIZE,\n",
    "          callbacks=[model_checkpoint,early_stopping])\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.axis([0, 500, 0, 1])\n",
    "\n",
    "model.load_weights(\"checkpoint.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "effde668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 995us/step\n"
     ]
    }
   ],
   "source": [
    "#Make our predictions and submit\n",
    "submission = pd.DataFrame()\n",
    "submission['PassengerId'] = test_df_par['PassengerId']\n",
    "submission['Survived'] = model.predict(X_test)\n",
    "submission['Survived'] = [1 if x >= 0.5 else 0 for x in submission['Survived']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
