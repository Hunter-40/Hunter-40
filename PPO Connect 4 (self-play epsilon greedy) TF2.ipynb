{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3e8e1876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import math\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "556dacc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = 7\n",
    "ROWS = 6\n",
    "EPISODES = 10000\n",
    "GAMES_PER_BATCH = 6\n",
    "UPDATE_EVRY = 600\n",
    "ESILON_DECAY = 0.99\n",
    "epsilon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "661307b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connect4Env(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete(COLS)\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(ROWS,COLS), dtype=np.int32)\n",
    "    def step(self, col):\n",
    "        if self.done:\n",
    "            print('Game is over must. Reset enviorment!!!')\n",
    "            return\n",
    "        if self.board[0][col] != 0:\n",
    "            self.done = True\n",
    "            self.reward = -2\n",
    "            return [self.board, self.reward, self.done]\n",
    "\n",
    "        self.board = self.board * -1\n",
    "        \n",
    "        row = None\n",
    "        for r in range(ROWS-1,-1,-1):\n",
    "            if self.board[r][col] == 0:\n",
    "                self.board[r][col] = 1\n",
    "                row = r \n",
    "                break\n",
    "        #Horizontal\n",
    "        connected = 0\n",
    "        for c in range(col+1, COLS):\n",
    "            if self.board[row][c] != 1:\n",
    "                break\n",
    "            connected += 1\n",
    "        for c in range(col-1, -1, -1):\n",
    "            if self.board[row][c] != 1:\n",
    "                break\n",
    "            connected += 1\n",
    "        if connected >= 3:\n",
    "            self.done = True\n",
    "            self.reward = 1\n",
    "            return [self.board, self.reward, self.done]\n",
    "        \n",
    "        #Vertical\n",
    "        connected = 0\n",
    "        for r in range(row+1, ROWS):\n",
    "            if self.board[r][col] != 1:\n",
    "                    break\n",
    "            connected += 1\n",
    "        for r in range(row-1, -1, -1):\n",
    "            if self.board[r][col] != 1:\n",
    "                break\n",
    "            connected += 1\n",
    "        if connected >= 3:\n",
    "            self.done = True\n",
    "            self.reward = 1\n",
    "            return [self.board, self.reward, self.done] \n",
    "        \n",
    "        #Positive Diagonal\n",
    "        connected = 0\n",
    "        for x in range(1, min(ROWS,COLS)):\n",
    "            if row + x > ROWS-1 or col + x > COLS-1:\n",
    "                break\n",
    "            if self.board[row+x][col+x] != 1:\n",
    "                break\n",
    "            connected += 1\n",
    "        for x in range(1, min(ROWS,COLS)):\n",
    "            if row - x < 0 or col - x < 0:\n",
    "                break\n",
    "            if self.board[row-x][col-x] != 1:\n",
    "                break\n",
    "            connected += 1\n",
    "        if connected >= 3:\n",
    "            self.done = True\n",
    "            self.reward = 1\n",
    "            return [self.board, self.reward, self.done]\n",
    "        \n",
    "        #Negative Digaonal\n",
    "        connected = 0\n",
    "        for x in range(1, min(ROWS,COLS)):\n",
    "            if row + x > ROWS-1 or col - x < 0:\n",
    "                break\n",
    "            if self.board[row+x][col-x] != 1:\n",
    "                break\n",
    "            connected += 1\n",
    "        for x in range(1, min(ROWS,COLS)):\n",
    "            if row - x < 0 or col + x > COLS-1:\n",
    "                break\n",
    "            if self.board[row-x][col+x] != 1:\n",
    "                break\n",
    "            connected += 1\n",
    "        if connected >= 3:\n",
    "            self.done = True\n",
    "            self.reward = 1\n",
    "            return [self.board, self.reward, self.done]\n",
    "        \n",
    "        #Full board\n",
    "        self.done = True\n",
    "        for i in range(COLS):\n",
    "            if self.board[0][i] == 0:\n",
    "                self.done = False\n",
    "                break\n",
    "        return [self.board, self.reward, self.done]\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.turn = 1\n",
    "        self.board = np.zeros((ROWS,COLS))\n",
    "        return self.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "40064c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.i = tf.keras.layers.Input(shape=(ROWS,COLS,1))\n",
    "        mask = np.asarray([c != c for c in range(COLS)], dtype=bool)\n",
    "        \n",
    "        self.shared_layers = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(512,(3,3),padding ='same'),\n",
    "            tf.keras.layers.Normalization(axis=None),\n",
    "            tf.keras.layers.Activation(partial(tf.nn.leaky_relu, alpha=0.01)),\n",
    "\n",
    "            tf.keras.layers.Conv2D(512,(3,3),padding ='same'),\n",
    "            tf.keras.layers.Normalization(axis=None),\n",
    "            tf.keras.layers.Activation(partial(tf.nn.leaky_relu, alpha=0.01)),\n",
    "\n",
    "            tf.keras.layers.Conv2D(512,(3,3)),\n",
    "            tf.keras.layers.Normalization(axis=None),\n",
    "            tf.keras.layers.Activation(partial(tf.nn.leaky_relu, alpha=0.01)),\n",
    "\n",
    "            tf.keras.layers.Conv2D(512,(3,3)),\n",
    "            tf.keras.layers.Normalization(axis=None),\n",
    "            tf.keras.layers.Activation(partial(tf.nn.leaky_relu, alpha=0.01)),\n",
    "            \n",
    "            tf.keras.layers.Flatten(),\n",
    "\n",
    "            tf.keras.layers.Dense(3072, activation=partial(tf.nn.leaky_relu, alpha=0.01)),\n",
    "            tf.keras.layers.Dense(1024, activation=partial(tf.nn.leaky_relu, alpha=0.01))\n",
    "        ])\n",
    "\n",
    "        self.value_head = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(512, activation=partial(tf.nn.leaky_relu, alpha=0.01)),\n",
    "            tf.keras.layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "\n",
    "        self.policy_head = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(512, activation=partial(tf.nn.leaky_relu, alpha=0.01)),\n",
    "            tf.keras.layers.Dense(COLS)\n",
    "        ])\n",
    "        \n",
    "        self.masking = tf.keras.layers.Softmax()\n",
    "        \n",
    "        z = self.shared_layers(self.i)\n",
    "        value = self.value_head(z)\n",
    "        p = self.policy_head(z)\n",
    "        policy = self.masking(p, mask = mask)\n",
    "        self.model = tf.keras.models.Model(inputs=self.i, outputs=[policy,value])\n",
    "        \n",
    "    def value(self, state):\n",
    "        z = self.shared_layers(state)\n",
    "        value = self.value_head(z)\n",
    "        return value\n",
    "    \n",
    "    def policy(self, state):\n",
    "        mask = IllegalMoveMask(state)\n",
    "        z = self.shared_layers(state)\n",
    "        p = self.policy_head(z)\n",
    "        policy = self.masking(p, mask = mask)\n",
    "        return policy\n",
    "    \n",
    "    def un_masked_policy(self, state):\n",
    "        z = self.shared_layers(state)\n",
    "        p = self.policy_head(z)\n",
    "        policy = self.masking(p)\n",
    "        return policy\n",
    "    \n",
    "    def value_policy(self, state):\n",
    "        mask = IllegalMoveMask(state)\n",
    "        z = self.shared_layers(state)\n",
    "        value = self.value_head(z)\n",
    "        p = self.policy_head(z)\n",
    "        policy = self.masking(p, mask = mask)\n",
    "        return value, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b451e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IllegalMoveMask(states):\n",
    "    m = []\n",
    "    for i in range(len(states)):\n",
    "        m.append([(states[i][0][c])[0] == 0 for c in range(COLS)])\n",
    "    m = np.asarray(m, dtype=bool)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5cf53f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make this use a training and testing model and store training data\n",
    "def rollout(env, model_train, model_test):\n",
    "    \n",
    "    training_data = [[], [], [], [],  []]\n",
    "    \n",
    "    done = False\n",
    "    board = env.reset()\n",
    "    \n",
    "    #half of the time the test model goes first\n",
    "    if random.uniform(0, 1) > 0.5:\n",
    "        probs = model_test.policy(board.reshape(-1, *board.shape, 1))\n",
    "        action_dist = tfp.distributions.Categorical(probs=probs)\n",
    "        opp_action = action_dist.sample()[0]\n",
    "        board, opp_reward, done = env.step(opp_action)\n",
    "        \n",
    "    while not done:\n",
    "        val, probs = model_train.value_policy(board.reshape(-1, *board.shape, 1))\n",
    "        val = val.numpy()[0][0]\n",
    "        \n",
    "        #Training action\n",
    "        print(probs)\n",
    "        action_dist = tfp.distributions.Categorical(probs=probs)\n",
    "        \n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "            action = action_dist.sample()\n",
    "            action = action.numpy()[0]\n",
    "        else:\n",
    "            action = random.choice([c for c in range(COLS) if board[0][c] == 0])\n",
    "            \n",
    "        action_prob = action_dist.prob(action)\n",
    "        action_prob = action_prob.numpy()\n",
    "        save_board = board\n",
    "        board, reward, done = env.step(action)\n",
    "        \n",
    "        #if the game is not over opponent gets to move\n",
    "        if not done:\n",
    "            probs = model_test.policy(board.reshape(-1, *board.shape, 1))\n",
    "            action_dist = tfp.distributions.Categorical(probs=probs)\n",
    "            opp_action = action_dist.sample()[0]\n",
    "            board, opp_reward, done = env.step(opp_action)\n",
    "            \n",
    "        #if opponent won we lost\n",
    "        if opp_reward == 1:\n",
    "            reward = -1 \n",
    "        #if opponent made an illegal move we win\n",
    "        if opp_reward == -2:\n",
    "            reward = 1\n",
    "            \n",
    "            \n",
    "        for i, item in enumerate((save_board, action, reward, val, action_prob)):\n",
    "            training_data[i].append(item)\n",
    "            \n",
    "    training_data[3] = calculate_gaes(training_data[2],training_data[3])\n",
    "    training_data.append(discounted_reward(training_data[2]))\n",
    "\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fc1d2977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gaes(rewards, values, gamma=0.99, decay=0.97):\n",
    "    next_values = np.concatenate([values[1:],[0]])\n",
    "    deltas = [r+gamma*n_v - v for r,v,n_v in zip(rewards, values, next_values)]\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "        gaes.append(deltas[i] + gamma * decay * gaes[-1])\n",
    "    return(gaes[::-1])\n",
    "\n",
    "def discounted_reward(rewards, gamma=0.99):\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
    "    return new_rewards[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "54a9adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainer\n",
    "def Train_PPO(model, boards, actions, old_probs, gaes, disc_rewards, lr=0.0000005, epsilon=0.2, delta = 0.001, max_iter = 80):\n",
    "    \n",
    "    boards = tf.convert_to_tensor(boards)\n",
    "    gaes = tf.convert_to_tensor(gaes, dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(actions) \n",
    "    old_probs = tf.convert_to_tensor(old_probs, dtype=tf.float32)\n",
    "    old_probs = tf.maximum(old_probs,1e-10) #prvent div by 0\n",
    "    old_probs = tf.squeeze(old_probs,1)\n",
    "    \n",
    "    mask = IllegalMoveMask(tf.expand_dims(boards, -1))\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        with tf.GradientTape(persistent = True) as tape:\n",
    "            probs = model.un_masked_policy(tf.expand_dims(boards, -1))\n",
    "            probs = tf.maximum(probs*mask,1e-10)\n",
    "            dist = tfp.distributions.Categorical(probs=probs)\n",
    "            new_probs = dist.prob(actions)\n",
    "            new_probs = tf.maximum(new_probs,1e-10) #prvent div by 0\n",
    "            policy_ratio = new_probs/old_probs\n",
    "            ploicy_loss = -tf.math.minimum(policy_ratio*gaes,\n",
    "                                           tf.clip_by_value(policy_ratio, 1-epsilon, 1+epsilon)*gaes)\n",
    "            policy_loss = tf.math.reduce_mean(ploicy_loss)\n",
    "            \n",
    "            \n",
    "            new_vals = model.value(tf.expand_dims(boards, -1))\n",
    "            new_vals = tf.squeeze(new_vals,1)\n",
    "            value_loss = tf.keras.losses.MSE(new_vals, disc_rewards)\n",
    "            total_loss = policy_loss + value_loss\n",
    "                \n",
    "        gradients = tape.gradient(total_loss, tape.watched_variables())\n",
    "        tf.keras.optimizers.Adam(lr).apply_gradients(zip(gradients, tape.watched_variables()))\n",
    "        \n",
    "        #Check if the kl divergence is too much before applying change\n",
    "        r = tf.maximum(policy_ratio, 1e-10) #prvent log of 0\n",
    "        kl_div = tf.math.reduce_mean((r-1)-tf.math.log(r))      \n",
    "        print(kl_div)\n",
    "        if kl_div >= delta:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6bd3484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization\n",
    "ac = ActorCriticNetwork()\n",
    "ac_opp = ActorCriticNetwork()\n",
    "ac_opp.set_weights(ac.get_weights())\n",
    "env = Connect4Env()\n",
    "ep_i = 0\n",
    "epsilon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "accafa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.0128384e-13 2.1800549e-01 2.6325852e-02 6.5245187e-01 1.0321674e-01\n",
      "  7.7829991e-12 2.2539524e-17]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.7581080e-14 1.9245337e-01 2.3465682e-02 7.0393693e-01 8.0143988e-02\n",
      "  1.7612478e-12 2.4941847e-18]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.4581213e-13 3.9339980e-01 4.9212296e-02 4.7504917e-01 8.2338721e-02\n",
      "  1.1907458e-11 3.3682798e-17]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[4.2411773e-12 4.1564646e-01 7.0482858e-02 4.3176973e-01 8.2100891e-02\n",
      "  2.0803397e-10 2.4310998e-15]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[7.0287508e-12 4.3981269e-01 8.1903346e-02 4.1716504e-01 6.1118897e-02\n",
      "  3.2753225e-10 4.6908655e-15]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.3422461e-09 6.8792230e-01 1.9615321e-01 0.0000000e+00 1.1592444e-01\n",
      "  2.9177182e-08 3.7061612e-12]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[8.9347525e-09 6.1524612e-01 2.1552716e-01 0.0000000e+00 1.6922657e-01\n",
      "  1.4499597e-07 4.2309194e-11]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.1006455e-08 0.0000000e+00 5.0529641e-01 0.0000000e+00 4.9470338e-01\n",
      "  1.9912268e-07 4.2534264e-11]], shape=(1, 7), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [281]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m training_data \u001b[38;5;241m=\u001b[39m [[],[],[],[],[],[]]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(GAMES_PER_BATCH):\n\u001b[1;32m----> 5\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mac_opp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(training_data)):\n\u001b[0;32m      8\u001b[0m         training_data[i] \u001b[38;5;241m=\u001b[39m training_data[i] \u001b[38;5;241m+\u001b[39m new_data[i]\n",
      "Input \u001b[1;32mIn [137]\u001b[0m, in \u001b[0;36mrollout\u001b[1;34m(env, model_train, model_test)\u001b[0m\n\u001b[0;32m     14\u001b[0m     board, opp_reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(opp_action)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 17\u001b[0m     val, probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     val \u001b[38;5;241m=\u001b[39m val\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m#Training action\u001b[39;00m\n",
      "Input \u001b[1;32mIn [135]\u001b[0m, in \u001b[0;36mActorCriticNetwork.value_policy\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     69\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_head(z)\n\u001b[0;32m     70\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_head(z)\n\u001b[1;32m---> 71\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value, policy\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1013\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1014\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1017\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\activation\\softmax.py:99\u001b[0m, in \u001b[0;36mSoftmax.call\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msoftmax(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend.py:5039\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m   5025\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras.backend.softmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   5026\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m   5027\u001b[0m \u001b[38;5;129m@doc_controls\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_generate_docs\n\u001b[0;32m   5028\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   5029\u001b[0m   \u001b[38;5;124;03m\"\"\"Softmax of a tensor.\u001b[39;00m\n\u001b[0;32m   5030\u001b[0m \n\u001b[0;32m   5031\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5037\u001b[0m \u001b[38;5;124;03m      A tensor.\u001b[39;00m\n\u001b[0;32m   5038\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5039\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:3867\u001b[0m, in \u001b[0;36msoftmax_v2\u001b[1;34m(logits, axis, name)\u001b[0m\n\u001b[0;32m   3865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3866\u001b[0m   axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 3867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrap_2d_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_nn_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:3785\u001b[0m, in \u001b[0;36m_wrap_2d_function\u001b[1;34m(inputs, compute_op, dim, name)\u001b[0m\n\u001b[0;32m   3782\u001b[0m is_last_dim \u001b[38;5;241m=\u001b[39m (dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (dim \u001b[38;5;241m==\u001b[39m shape\u001b[38;5;241m.\u001b[39mndims \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   3784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_last_dim:\n\u001b[1;32m-> 3785\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3787\u001b[0m dim_val \u001b[38;5;241m=\u001b[39m dim\n\u001b[0;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dim, ops\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:10927\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(logits, name)\u001b[0m\n\u001b[0;32m  10925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m  10926\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 10927\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10928\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSoftmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m  10930\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "for _ in range(EPISODES):\n",
    "    training_data = [[],[],[],[],[],[]]\n",
    "    for _ in range(GAMES_PER_BATCH):\n",
    "        new_data = rollout(env, ac, ac_opp)\n",
    "     \n",
    "        for i in range(len(training_data)):\n",
    "            training_data[i] = training_data[i] + new_data[i]\n",
    "            \n",
    "    epsilon = epsilon*ESILON_DECAY \n",
    "    \n",
    "    #Shuffle the training data\n",
    "    s = random.randint(0,100000)#Any big number\n",
    "    training_data = [random.Random(s).sample(x,len(training_data[0])) for x in training_data]\n",
    "\n",
    "\n",
    "    #Train PPO\n",
    "    Train_PPO(ac, training_data[0], training_data[1], training_data[4], training_data[3], training_data[5])\n",
    "\n",
    "    ep_i+=1\n",
    "    if not ep_i % UPDATE_EVRY:     \n",
    "        ac_opp.set_weights(ac.get_weights())\n",
    "        ac.save_weights(\"Episode-{}-model-weights\".format(ep_i))\n",
    "        epsilon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "e639559c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "tf.Tensor(\n",
      "[[1.2814886e-13 4.0082127e-01 8.5881921e-03 5.8045435e-01 1.0136227e-02\n",
      "  6.8818085e-12 2.3772584e-17]], shape=(1, 7), dtype=float32)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[2.1340839e-14 3.5317177e-01 6.7706197e-03 6.3308042e-01 6.9771996e-03\n",
      "  1.4755902e-12 2.4656325e-18]], shape=(1, 7), dtype=float32)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[6.7487011e-14 4.1808000e-01 9.5635783e-03 5.6674594e-01 5.6105088e-03\n",
      "  4.1028973e-12 1.0488740e-17]], shape=(1, 7), dtype=float32)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[2.1479866e-12 6.2579072e-01 2.0542813e-02 3.4608585e-01 7.5806081e-03\n",
      "  8.5319772e-11 8.6114277e-16]], shape=(1, 7), dtype=float32)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[2.3917656e-12 6.7952967e-01 2.1621741e-02 2.9139459e-01 7.4540251e-03\n",
      "  9.3373670e-11 9.7574574e-16]], shape=(1, 7), dtype=float32)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[9.2824316e-13 6.7719662e-01 1.9961284e-02 2.9713014e-01 5.7120104e-03\n",
      "  4.1913042e-11 2.8627093e-16]], shape=(1, 7), dtype=float32)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[9.8908277e-12 7.0318902e-01 2.4577927e-02 2.6289037e-01 9.3427515e-03\n",
      "  3.1475836e-10 5.9464838e-15]], shape=(1, 7), dtype=float32)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[5.5672793e-11 5.8699054e-01 2.8133418e-02 3.7012771e-01 1.4748310e-02\n",
      "  1.3939001e-09 5.6088272e-14]], shape=(1, 7), dtype=float32)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[6.7309591e-10 5.1817399e-01 3.6781374e-02 4.1806263e-01 2.6982026e-02\n",
      "  1.2020795e-08 1.4614711e-12]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[9.8178254e-10 0.0000000e+00 6.3892193e-02 8.8955098e-01 4.6556804e-02\n",
      "  1.7898065e-08 1.9585360e-12]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[2.4640981e-08 0.0000000e+00 1.6775280e-01 7.5554049e-01 7.6706424e-02\n",
      "  3.1454394e-07 1.1172946e-10]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[2.7606697e-07 0.0000000e+00 2.4985752e-01 6.3468808e-01 1.1545159e-01\n",
      "  2.4962762e-06 2.4726596e-09]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[1.0524869e-06 0.0000000e+00 7.0397049e-01 0.0000000e+00 2.9601923e-01\n",
      "  9.1620468e-06 1.0988758e-08]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[3.9597242e-07 0.0000000e+00 7.1132725e-01 0.0000000e+00 2.8866839e-01\n",
      "  3.9712609e-06 2.8834599e-09]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[2.4637060e-07 0.0000000e+00 7.8145963e-01 0.0000000e+00 2.1853745e-01\n",
      "  2.6947434e-06 1.6042360e-09]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[2.2950367e-07 0.0000000e+00 7.5466907e-01 0.0000000e+00 2.4532808e-01\n",
      "  2.5523987e-06 1.3921151e-09]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[6.4475313e-07 0.0000000e+00 7.6749551e-01 0.0000000e+00 2.3249777e-01\n",
      "  6.0457946e-06 5.4534026e-09]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[1.36702192e-06 0.00000000e+00 6.93669617e-01 0.00000000e+00\n",
      "  3.06317776e-01 1.11543886e-05 1.45340096e-08]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[8.5270622e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9992597e-01\n",
      "  6.5287386e-05 1.1230000e-07]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1. -1.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[2.6169607e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9997485e-01\n",
      "  2.2540225e-05 2.3636430e-08]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  1.  0.  0.]\n",
      " [ 0.  1. -1. -1. -1.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[2.4815854e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9997473e-01\n",
      "  2.2708993e-05 2.1040325e-08]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1. -1.  0.  0.]\n",
      " [ 0. -1.  1.  1.  1.  0.  0.]\n",
      " [ 0.  1. -1. -1. -1.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[2.0810851e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9997842e-01\n",
      "  1.9608960e-05 1.6014754e-08]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  1.  0.  0.]\n",
      " [ 0.  1. -1. -1. -1.  0.  0.]\n",
      " [ 0. -1.  1.  1.  1.  0.  0.]\n",
      " [ 0.  1. -1. -1. -1.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[6.0373695e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9994159e-01\n",
      "  5.2187308e-05 6.2303350e-08]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1. -1.  0.  0.]\n",
      " [ 0. -1.  1.  1.  1.  0.  0.]\n",
      " [ 0.  1. -1. -1. -1.  0.  0.]\n",
      " [ 0. -1.  1.  1.  1.  0.  0.]\n",
      " [ 0.  1. -1. -1. -1.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[8.90606316e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  9.99918222e-01 7.26869912e-05 1.03213175e-07]], shape=(1, 7), dtype=float32)\n",
      "[[ 0. -1.  1.  1.  1.  0.  0.]\n",
      " [ 0.  1. -1. -1. -1.  0.  0.]\n",
      " [ 0. -1.  1.  1.  1.  0.  0.]\n",
      " [ 0.  1. -1. -1. -1.  0.  0.]\n",
      " [ 0. -1.  1.  1.  1.  0.  0.]\n",
      " [ 0.  1. -1. -1. -1.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[0.11189976 0.         0.         0.         0.         0.8866414\n",
      "  0.00145882]], shape=(1, 7), dtype=float32)\n",
      "[[-0.  1. -1. -1. -1. -0. -0.]\n",
      " [-0. -1.  1.  1.  1. -0. -0.]\n",
      " [-0.  1. -1. -1. -1. -0. -0.]\n",
      " [-0. -1.  1.  1.  1. -0. -0.]\n",
      " [-0.  1. -1. -1. -1. -0. -0.]\n",
      " [-0. -1.  1.  1.  1.  1. -0.]]\n"
     ]
    }
   ],
   "source": [
    "#Show game in terminal\n",
    "done = False\n",
    "board = env.reset()\n",
    "t = 1\n",
    "while not done:\n",
    "    print(board*t)\n",
    "    probs = ac.policy(board.reshape(-1, *board.shape, 1))\n",
    "    print(probs)\n",
    "    action = np.argmax(probs)\n",
    "    board, reward, done = env.step(action)\n",
    "    if done:\n",
    "        print(board)\n",
    "    t = t * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "88e46567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load saved weights\n",
    "ep_i = 300\n",
    "ac.load_weights(\"Episode-{}-model-weights\".format(ep_i))\n",
    "ac_opp.load_weights(\"Episode-{}-model-weights\".format(ep_i))\n",
    "epsilon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "e5cc82f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#play against the model\n",
    "done = False\n",
    "board = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "08fb85fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4.9855975e-10 6.1719155e-01 8.1421666e-02 3.0138665e-01 0.0000000e+00\n",
      "  1.6651690e-07 5.5534349e-12]], shape=(1, 7), dtype=float32)\n",
      "[[ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0. -1.  0.  0.]\n",
      " [ 0. -1.  0.  0.  1.  0.  0.]\n",
      " [-1. -1.  0.  0.  1.  0.  0.]\n",
      " [-1. -1. -1.  0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "probs = ac.policy(board.reshape(-1, *board.shape, 1))\n",
    "print(probs)\n",
    "action = np.argmax(probs)\n",
    "board, reward, done = env.step(action)\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e05d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "board, reward, done = env.step(2)\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73240937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
